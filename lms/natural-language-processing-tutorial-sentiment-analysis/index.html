
<!doctype html>

<html>
<head>
  <meta name="viewport" content="width=device-width, minimum-scale=1.0, initial-scale=1.0, user-scalable=yes">
  <meta name="theme-color" content="#4F7DC9">
  <meta charset="UTF-8">
  <title>Natural Language Processing Tutorial - Sentiment Analysis</title>
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Source+Code+Pro:400|Roboto:400,300,400italic,500,700|Roboto+Mono">
  <link rel="stylesheet" href="//fonts.googleapis.com/icon?family=Material+Icons">
  <link rel="stylesheet" href="../assets/codelab-elements/codelab-elements.css">
  <link rel="stylesheet" href="custom.css">
  <style>
    .success {
      color: #1e8e3e;
    }
    .error {
      color: red;
    }
  </style>
</head>
<body>
  <google-codelab-analytics gaid="UA-49880327-14"></google-codelab-analytics>
  <google-codelab title="Natural Language Processing Tutorial - Sentiment Analysis"
                  environment="web"
                  feedback-link="">
    
      <google-codelab-step label="Objective" duration="0">
        <p>Sentiment analysis, also known as opinion mining is a subfield of Natural Language Processing (NLP) that tries to identify and extract opinions from a given text. The aim of sentiment analysis is to gauge the attitudes, sentiments, and emotions of a speaker/writer based on the computational treatment of subjectivity in a text. This can be in the form of like/dislike binary rating or in the form of numerical ratings from 1 to 5.</p>
<p><img alt="sentiment-analysis" src="img/ecc3ddedae3c6e65.jpg"></p>
<p><a href="https://www.kdnuggets.com/images/sentiment-fig-1-689.jpg" target="_blank">Image source</a></p>
<p>Sentiment Analysis is an important sub-field of NLP. It can help to create targeted brand messages and assist a company in understanding consumer&#39;s preferences. These insights could be critical for a company to increase its reach and influence across a range of sectors.</p>
<p>Here are some of the uses of Sentiment analysis from a business perspective:</p>
<p><img alt="sentiment-analysis-business-uses" src="img/422ae4d8e742a3e9.jpg"></p>
<p>In this tutorial, you will learn how to apply automatic machine learning to build a model to classify customer reviews. You will learn some core NLP concepts and then load a dataset, explore it, run an experiment to build a model and explore the results.</p>
<p><strong>Note</strong>: It is highly recommended that you go over the entire tutorial before starting the experiment. This would help you become more familiar with the content and aid you while you are performing the experiment..</p>


      </google-codelab-step>
    
      <google-codelab-step label="Prerequisites" duration="0">
        <p>You will need the following to be able to do this tutorial:</p>
<ul>
<li>Basic knowledge of Machine Learning and Statistics<br></li>
<li>A hands on knowledge of Driverless AI environment</li>
<li>Basic knowledge of Driverless AI or doing the <a href="https://training.h2o.ai/products/tutorial-1a-automatic-machine-learning-introduction-with-driverless-ai" target="_blank">Automatic Machine Learning Introduction with Drivereless AI</a></li>
<li>A <strong>Two-Hour Test Drive session</strong> : Test Drive is H2O.ai&#39;s Driverless AI on the AWS Cloud. No need to download software. Explore all the features and benefits of the H2O Automatic Learning Platform.<ul>
<li>Need a <strong>Two-Hour Test Drive</strong> session? Follow the instructions on <a href="https://training.h2o.ai/products/tutorial-0-getting-started-with-driverless-ai-test-drive" target="_blank">this quick tutorial</a> to get a Test Drive session started.<br><br></li>
</ul>
</li>
</ul>
<p><strong>Note:  Aquarium&#39;s Driverless AI Test Drive lab has a license key built-in, so you don&#39;t need to request one to use it. Each Driverless AI Test Drive instance will be available to you for two hours, after which it will terminate. No work will be saved. If you need more time to further explore Driverless AI, you can always launch another Test Drive instance or reach out to our sales team via the </strong><a href="https://www.h2o.ai/company/contact/" target="_blank"><strong>contact us form</strong></a><strong>.</strong></p>


      </google-codelab-step>
    
      <google-codelab-step label="Task 1: Launch Sentiment Analysis Experiment" duration="0">
        <h2>About the Dataset</h2>
<p>The dataset consists of reviews of fine foods from <a href="https://www.amazon.com/" target="_blank">Amazon</a>. The data spans a period of more than 10 years, from Oct 1999 up to October 2012. Reviews include product and user information, ratings, and a plain text review. It also includes reviews from all other Amazon categories[1]. The data consists of 568,454 reviews, 256,059 users, 74,258 products and 260 users with &gt; 50 reviews.</p>
<p>Our aim is to study these reviews and try and predict whether a review is positive or negative.</p>
<p>The data has been originally hosted by SNAP (<a href="http://snap.stanford.edu/data/index.html" target="_blank">Stanford Large Network Dataset Collection</a>), a collection of more than 50 large network datasets from tens of thousands of nodes and edges to tens of millions of nodes and edges. In includes social networks, web graphs, road networks, internet networks, citation networks, collaboration networks, and communication networks [2].</p>
<p>The dataset provided is a CSV file containing 568,454 rows and a total of 10 features (columns).</p>
<h2>Dataset Overview</h2>
<p>If you are using <strong>Aquarium</strong> as the environment, then the lab <strong>Driverless AI Test Drive</strong> has the Amazon fine review dataset already pre-loaded with separate training and test datasets. The datasets can be located on the Datasets Overview page. However, you can also upload the datasets externally. To learn more about how to add the two datasets from the Driverless AI file system then see Appendix A: Add the Datasets.</p>
<p>On clicking the highlighted <code>Start lab</code> button , you will be taken  to a Driverless AI platform with several pre-loaded data sets and pre-run visualizations, models, interpretations, and deployments. Here  you will have access to both the training and testing set for Amazon fine food reviews.</p>
<p>1. Verify that both datasets are on the Datasets Overview, your screen should look similar to the page below:</p>
<p><img alt="datasets-overview" src="img/6c94a841d14c021c.jpg"></p>
<p>2. Click on the <code>AmazonFineFood_train</code> file and then on Details.</p>
<p><img alt="dataset-details" src="img/68d9049c92bf2440.jpg"></p>
<p>3. Let&#39;s take a quick look at the columns of the training set:</p>
<p><img alt="dataset-columns-info" src="img/4ada5ba77b3602d4.jpg"></p>
<p><em>Things to Note:</em></p>
<p>The dataset consists of 10 columns which are as follows:</p>
<ol type="1">
<li><strong>UserId</strong> - Unique identifier for the user</li>
<li><strong>ProductId</strong> - Unique identifier for the product</li>
<li><strong>Id</strong> - Row Id</li>
<li><strong>Summary</strong> - Brief summary of the review</li>
<li><strong>Score</strong> - Rating between 1 and 5</li>
<li><strong>HelpfulnessDenominator</strong> - Number of users who indicated whether they found the review helpful or not</li>
<li><strong>ProfileName</strong> - Profile name of the user<br></li>
</ol>
<p>4. Continue scrolling the current page to see more columns (image is not included)</p>
<ol type="1">
<li><strong>HelpfulnessNumerator</strong> - Number of users who found the review helpful</li>
<li><strong>Time</strong> - Timestamp for the review</li>
<li><strong>Description</strong> - Text of the review</li>
<li><strong>Positive Review</strong> - Whether the review is Positive or Negative</li>
</ol>
<p>5. Return to the <strong>Datasets</strong> Page</p>
<h2>Launch Experiment</h2>
<p>1. On the Datasets page click on the <code>**AmazonFineFoodReviews-train-26k.csv**</code> dataset and select Predict</p>
<p><img alt="launch-experiment" src="img/e71f2c2268598c56.jpg"></p>
<p>2. As soon as you select the <code>Predict</code> option, you are asked for a tour of Driverless AI environment. Skip it for now by clicking <code>Not Now</code>. However, it is recommended to get the intuitive tour incase you are new to the environment. An image similar to the one below should appear.</p>
<p><img alt="initial-experiment-overview" src="img/24288d272ac82022.jpg"></p>
<p>3. Next, you will need to feed in the following information into Driverless AI :</p>
<p><img alt="name-experiment" src="img/5e352820a577ebd7.jpg"></p>
<ol type="1">
<li><strong>Display Name</strong> - Give a name to your experiment. You can choose according to your choice but it is recommended to choose a name that highlights the purpose of the experiment. Let&#39;s name our current experiment as <strong>Sentiment Analysis</strong>.</li>
<li><strong>Target Column</strong> -  The <em>Target column</em> contains the value that we intend to predict through the experiment. Click on the <code>Select Target Column</code> tab and select <strong>Positive Review</strong> as the target column. The aim of the experiment is to try to predict whether a fiven review is positive or negative, hence the Positive Review is selected as the target column. The column has only two values i.e Positive and Negative.<img alt="target-column" src="img/ecff5842c3b2533e.jpg"></li>
<li><strong>Dropped Columns</strong> - The <em>Dropped Columns</em> feature enable us to  drop column(s) from your dataset that you don&#39;t want  to use in the experiment.For this experiment we shall only use the text columns so we shall drop all columns that are not in text format.<img alt="dropped-columns" src="img/7881e25fb41b758c.jpg">However, please note that if you decide to keep the non-text columns, the NLP algorithms will still work on the non-text columns.</li>
<li><strong>Test Dataset</strong> -  The <em>Test dataset</em> is a dataset used to provide an unbiased evaluation of a <em>final</em> model fit on the training dataset. It is not used during training of the model and results are available at the end of the experiment. Select the <code>AmazonFineFoodReviews-test-26k.csv</code> dataset as follows:<img alt="test-dataset" src="img/dd39793e5d6a91f6.jpg">The experiment screen will finally look like the image below:<img alt="final-experiment-screen" src="img/b471480faf1aa53d.jpg"></li>
</ol>
<p>In <strong>Task 2</strong>, we shall explore and update the Experiment Settings.</p>
<h2>Acknowledgement</h2>
<ul>
<li><a href="http://i.stanford.edu/~julian/pdfs/www13.pdf" target="_blank">J. McAuley and J. Leskovec. From amateurs to connoisseurs: modeling the evolution of user expertise through online reviews. WWW, 2013.</a></li>
</ul>
<h2>References</h2>
<p>[1] <a href="https://www.kaggle.com/snap/amazon-fine-food-reviews" target="_blank">Amazon Fine Food Reviews - Analyze ~500,000 food reviews from Amazon</a></p>
<p>[2] <a href="https://www.kaggle.com/snap" target="_blank">Stanford Network Analysis Project</a></p>
<h2>Deeper Dive and Resources</h2>
<ul>
<li><a href="http://docs.h2o.ai/driverless-ai/latest-stable/docs/userguide/nlp.html" target="_blank">NLP in Driverless AI documentation</a></li>
</ul>


      </google-codelab-step>
    
      <google-codelab-step label="Task 2: Sentiment Analysis Experiment Settings" duration="0">
        <p>Once the data has been imported into Driverless AI, there are certain experiment settings that need to be updated. This section deals with experiment settings with respect to NLP tasks. However, if you wish to learn more about the meaning and various experimental settings in general, It is recommended to go through the following tutorial:</p>
<ul>
<li><a href="https://training.h2o.ai/products/tutorial-1b-machine-learning-experiment-scoring-and-analysis-tutorial-financial-focus" target="_blank">Machine Learning Experiment Scoring and Analysis Tutorial - Financial Focus</a><br></li>
</ul>
<p>Experiment Settings describe the Accuracy, Time and Interpretability of a specific experiment. The knobs on the experiment settings are adjustable. As values change, the meaning of the settings on the left-bottom page change.</p>
<p>Here are the following settings that need to be updated for a typical NLP experiment.</p>
<p><img alt="experiment-settings" src="img/26402681970021c4.jpg"></p>
<ol type="1">
<li>Accuracy</li>
<li>Time</li>
<li>Interpretability</li>
<li>Scorer</li>
<li>Expert Settings</li>
</ol>
<h2>Accuracy</h2>
<p>Accuracy stands for relative accuracy i.e higher values, should lead to higher confidence in model performance (accuracy). The accuracy setting impacts which algorithms are considered, level of assembling and types of feature engineering,</p>
<h2>Time</h2>
<p>Time is the Relative time for completing the experiment. Higher values will take longer for the experiment to complete.</p>
<h2>Interpretability</h2>
<p>Interpretability is the degree to which a human can understand the cause of the decision. It controls the complexity of the models and features allowed within the experiments (e.g. higher interpretability will generally block complicated features, feature engineering, and models).</p>
<h2>Scorer</h2>
<p>Scorer is the metric used to Evaluate the machine learning algorithm. The scorer used for this experiment is the LogLoss or logarithmic loss metric which is used to used to evaluate the performance of a binomial or multinomial classifier. Unlike AUC which looks at how well a model can classify a binary target, logloss evaluates how close a model&#39;s predicted values (uncalibrated probability estimates) are to the actual target value. The lower the Logloss value the better the better the model can predict the sentiment.</p>
<h2>Expert Settings</h2>
<p>Several configurable settings are available for NLP in Driverless AI, which can be tuned according to the experiment type. To tune the NLP settings, click on the Expert Settings, and navigate to the NLP option:</p>
<p><img alt="expert-settings-overview" src="img/23e5d37ffa976130.jpg"></p>
<p>Click on the NLP tab to enable NLP specific settings as shown below. This option allows you to enable the following options for NLP experiments.</p>
<ul>
<li><strong>Word-Based CNN TensorFlow Models for NLP</strong></li>
</ul>
<p>Specify whether to use Word-based CNN TensorFlow models for NLP. This option is ignored if TensorFlow is disabled. We recommend that you disable this option on systems that do not use GPUs.</p>
<ul>
<li><strong>Word-Based BiGRU TensorFlow Models for NLP</strong></li>
</ul>
<p>Specify whether to use Word-based BiG-RU TensorFlow models for NLP. This option is ignored if TensorFlow is disabled. We recommend that you disable this option on systems that do not use GPUs.</p>
<ul>
<li><strong>Character-Based CNN TensorFlow Models for NLP</strong></li>
</ul>
<p>Specify whether to use Character-level CNN TensorFlow models for NLP. This option is ignored if TensorFlow is disabled. We recommend that you disable this option on systems that do not use GPUs.</p>
<ul>
<li><strong>PyTorch Models for NLP (Experimental)</strong></li>
</ul>
<p>Specify whether to enable pre-trained PyTorch models and fine-tune them for NLP tasks. This is set to Auto by default. You need to set this to On if you want to use the PyTorch models like BERT for feature engineering or for modeling. We recommend that you use GPUs to speed up execution when this option is used.</p>
<p><img alt="nlp-expert-settings" src="img/99eabd157da24336.jpg"></p>
<ul>
<li><strong>Select Which Pretrained PyTorch NLP Models to Use</strong></li>
</ul>
<p>Click on the <code>Select Which Pretrained PyTorch NLP Models to Use</code> and specify one or more pretrained PyTorch NLP models to use from the following list:</p>
<p><img alt="pytorch-pretrained-models" src="img/5b6668073f9a5a00.jpg"></p>
<h3>Notes:</h3>
<ul>
<li>This setting requires an Internet connection.</li>
<li>Using BERT-like models may result in a longer experiment completion time.</li>
</ul>
<p>Additionally, there are three more buttons located beneath the experimental settings knob which stand for the following:</p>
<ul>
<li><strong>Classification or Regressio</strong>: Driverless AI automatically determines the problem type based on the response column. Though not recommended, you can override this setting by clicking this button. Our current problem is that of Classification.</li>
<li><strong>Reproducible</strong>: This button allows you to build an experiment with a random seed and get reproducible results. If this is disabled (default), the results will vary between runs.</li>
<li><strong>Enable GPUs</strong>: Specify whether to enable GPUs. (Note that this option is ignored on CPU-only systems.)</li>
</ul>
<p>Update the following experiment settings so that they match the image below, then select Launch Experiment. This configuration is selected to generate a model quickly with a sufficient level of accuracy in the H2O Driverless Test Drive environment.</p>
<ul>
<li><strong>Accuracy</strong>: 5</li>
<li><strong>Time</strong>: 2</li>
<li><strong>Interpretability</strong>: 5</li>
<li><strong>Scorer</strong>: LogLoss</li>
</ul>
<p><img alt="final-experiment-launch" src="img/563606daf664786f.jpg"></p>
<p>The time to run this experiment depends upon the memory, availability of GPU in a system and the expert settings which have been selected by the user. If the system does not have a GPU, it might run for a longer time.</p>
<h2>Resources</h2>
<p>[1] <a href="http://statweb.stanford.edu/~jhf/ftp/RuleFit.pdf" target="_blank">J. Friedman, B. Popescu. &#34;Predictive Learning via Rule Ensembles&#34;. 2005</a></p>
<p>[2] <a href="https://en.wikipedia.org/wiki/Ensemble_learning" target="_blank">Ensemble Learning</a></p>
<h2>Deeper Dive</h2>
<ul>
<li><a href="http://docs.h2o.ai/driverless-ai/latest-stable/docs/userguide/experiment-settings.html?highlight=interpretability#accuracy-time-and-interpretability-knobs" target="_blank">To better understand the impact of setting the Accuracy, Time and Interpretability Knobs between 1 and 10 in H2O Driverless AI</a></li>
<li><a href="http://docs.h2o.ai/driverless-ai/latest-stable/docs/userguide/expert-settings.html?highlight=expert%20settings" target="_blank">Expert Settings for H2O Driverless AI</a></li>
</ul>


      </google-codelab-step>
    
      <google-codelab-step label="Task 3: Natural Language Processing Concepts" duration="0">
        <h2>Natural Language Processing(NLP)</h2>
<p>NLP is the field of study that focuses on the interactions between human language and computers. NLP sits at the intersection of computer science, artificial intelligence, and computational linguistics[1]. NLP is a way for computers to analyze, understand, and derive meaning from human language in a smart and useful way. By utilizing NLP, developers can organize and structure knowledge to perform tasks such as :</p>
<ul>
<li>Automatic tagging of incoming customer queries related to credit card, loans etc</li>
<li>Sentiment analysis of Social media reviews</li>
<li>Using free text variables along with numeric variables for credit risk, fraud models.</li>
<li>Emotion detection</li>
<li>Profanity detection</li>
</ul>
<p>The text data is highly unstructured but the Machine learning algorithms usually work with input features that are numeric in nature. So before we start with any NLP project we need to pre-process and normalize the text to make it ideal for feeding into the commonly available Machine learning algorithms. This essentially means we need to build a pipeline of some sort that breaks down the problem into several pieces. We can then apply various methodologies on these pieces and plug the solution together in the form of a pipeline.</p>
<h2>Building a Typical NLP Pipeline</h2>
<p><img alt="nlp-pipeline" src="img/97b29e2b1deef178.jpg"></p>
<p>The figure above shows how a typical pipeline looks like. It is also important to note that there may be variations depending upon the problem at hand. Hence the pipeline will have to be adjusted to suit our needs. Driverless AI automates the above process. Let&#39;s try and understand some of the components of the pipeline in brief:</p>
<h2>Text preprocessing</h2>
<p>Text pre-processing involves using a variety of techniques to convert raw text into well-defined sequences of linguistic components that have standard structure and notation. Some of those techniques are:</p>
<ul>
<li><strong>Sentence Segmentation:</strong> involves identifying sentence boundaries between words in different sentences. Since most written languages have punctuation marks which occur at sentence boundaries, sentence segmentation is frequently referred to as sentence boundary detection, sentence boundary disambiguation, or sentence boundary recognition. All these terms refer to the same task: determining how a text should be divided into sentences for further processing.</li>
<li><strong>Text Tokenization:</strong> Tokenization involves splitting raw text corpus into sentences and then further splitting each sentence into words.</li>
<li><strong>Text Standardisation:</strong> Once the text has been tokenized, it is normalised by getting rid of the noise. Noise relates to anything that isn&#39;t in a standard format like punctuation marks, special characters or any unwanted tokens. If required, case conversions can also be done i.e converting all tokens into either lowercase or uppercase.</li>
<li><strong>Removing Stopwords:</strong> Stop words are words which appear very frequently in a text like &#34;and&#34;, &#34;the&#34;, and &#34;a&#34;, but appear to be of little value in helping select documents. Such words are excluded from the vocabulary entirely.</li>
<li><strong>Stemming:</strong> Stemming is the process of reducing inflected (or sometimes derived) words to their stem, base or root form — generally a written word form. Example: if we were to stem the following words: &#34;Stems&#34;, &#34;Stemming&#34;, &#34;Stemmed&#34;, &#34;and &#34;Stemtization&#34;, the result would be a single token &#34;stem&#34;.</li>
<li><strong>Lemmatization:</strong> a similar operation to stemming is lemmatization. However, the major difference between the two is that stemming can often create non-existent words, whereas lemmatization creates actual words. An example of lemmatization: &#34;run&#34; is a base form for words like &#34;running&#34; or &#34;ran&#34; and the word &#34;better&#34; and &#34;good&#34; are in the same lemma, so they are considered the same.</li>
</ul>
<p>It is important to note here that the above steps are not mandatory and their usage depends upon the use case. For instance, in sentiment analysis emoticons signify polarity and stripping them off from the text may not be a good idea. The general goal of the Normalization, Stemming and Lemmatization techniques is to improve the generalization of the model. Essentially we are mapping different variants of what we consider to be the same or very similar &#34;word&#34; to one token in our data.</p>
<h2>Feature Extraction</h2>
<p>The Machine Learning Algorithms usually expect features in the form of numeric vectors. Hence, after the initial preprocessing phase, we need to transform the text into a meaningful vector (or array) of numbers. This process is called <strong>feature extraction</strong>. Let&#39;s see how some of the feature-extracting techniques work.</p>
<ul>
<li><strong>Bag of Words:</strong> The bag-of-words is a representation of text that describes the occurrence of words within a document. It involves two things:<ul>
<li>A vocabulary of known words</li>
<li>A measure of the presence of known words</li>
</ul>
</li>
</ul>
<p>The intuition behind the Bag of Words is that documents are similar if they have identical content, and we can get an idea about the meaning of the document from its content alone.</p>
<h2><a href="https://en.wikipedia.org/wiki/Bag-of-words_model" target="_blank">Example implementation</a></h2>
<p>The following models a text document using bag-of-words. Here are two simple text documents:</p>
<ul>
<li>John likes to watch movies. Mary likes movies too.</li>
<li>John also likes to watch football games.</li>
</ul>
<p>Based on these two text documents, a list is constructed as follows for each document:</p>
<ul>
<li>&#34;John&#34;,&#34;likes&#34;,&#34;to&#34;,&#34;watch&#34;,&#34;movies&#34;,&#34;Mary&#34;,&#34;likes&#34;,&#34;movies&#34;,&#34;too&#34;</li>
<li>&#34;John&#34;,&#34;also&#34;,&#34;likes&#34;,&#34;to&#34;,&#34;watch&#34;,&#34;football&#34;,&#34;games&#34;</li>
</ul>
<p>Representing each bag-of-words as a JSON object, and attributing to the respective JavaScript variable:</p>
<ul>
<li>BoW1 = {&#34;John&#34;:1,&#34;likes&#34;:2,&#34;to&#34;:1,&#34;watch&#34;:1,&#34;movies&#34;:2,&#34;Mary&#34;:1,&#34;too&#34;:1};</li>
<li>BoW2 = {&#34;John&#34;:1,&#34;also&#34;:1,&#34;likes&#34;:1,&#34;to&#34;:1,&#34;watch&#34;:1,&#34;football&#34;:1,&#34;games&#34;:1};</li>
</ul>
<p>It is important to note that BoW does not retain word order and is sensitive towards document length, i.e token frequency counts could be higher for longer documents.</p>
<p>It is also possible to create BoW models with combination of consecutive words, also known as n-grams.</p>
<ul>
<li><strong>TF-IDF Model:</strong> A problem with the Bag of Words approach is that highly frequent words start to dominate in the document (e.g. larger score), but may not contain as much &#34;informational content&#34;. Also, it will give more weight to longer documents than the shorter ones. One approach is to rescale the frequency of words by how often they appear in all documents so that the scores for frequent words across all documents are penalized. This approach of scoring is called  <strong>Term Frequency-Inverse Document Frequency</strong>, or TF-IDF [2] for short, where:<ul>
<li><strong>Term Frequency</strong>  is a scoring of the frequency of the word in the current document. TF = (Number of times term t appears in a document)/(Number of terms in the document)</li>
<li><strong>Inverse Document Frequency</strong>: is a scoring of how rare the word is across documents.IDF = 1+log(N/n), where N is the number of documents and n is the number of documents a term t has appeared in. TF-IDF weight is often used in information retrieval and text mining. This weight is a statistical measure used to evaluate how important a word is to a document in a collection or corpus</li>
</ul>
</li>
</ul>
<p><img alt="tfidf" src="img/dcfd44a9173da5dd.jpg"></p>
<p>The dimensions of the output vectors are high. This also gives importance to the rare terms that occur in the corpus which might be helpful in our classification tasks.</p>
<ul>
<li><strong>Principal Component Analysis (PCA)</strong></li>
</ul>
<p>Principal Component Analysis s a dimension reduction tool that can be used to reduce a large set of variables to a small set that still contains most of the information in the original set</p>
<ul>
<li><strong>Truncated SVD<br></strong></li>
</ul>
<p>SVD stands for Singular Value Decomposition[3] which is a way to decompose matrices. Truncated SVD is a common method to reduce the dimension for text-based frequency/vectors.</p>
<ul>
<li><strong>Advanced Word Vectorization Models<br></strong></li>
</ul>
<p>TFIDF and frequency-based models represent counts and significant word information, but they lack semantics of the words in general. One of the popular representations of text to overcome this is Word Embeddings.</p>
<p>Word embeddings is a feature engineering technique for text where words or phrases from the vocabulary are mapped to vectors of real numbers. There are ways to create more advanced word vectorization models for extracting features from text data like word2vec[2] model. Released in 2013 by Google, word2vec is a neural network-based implementation that learns distributed vector representations of words based on continuous Bag of Words and skip-gram–based architectures</p>
<p><img alt="word-vectors" src="img/778c1b0e9843a293.jpg"></p>
<p>Representations are made in such a way that words that have similar meanings are placed close or equidistant to each other. For example, words like ‘king&#39; is closely associated with &#34;queen&#34; in this vector representation.</p>
<ul>
<li><strong>Convolution neural network(CNN) models on word embeddings</strong></li>
</ul>
<p>CNN&#39;s are generally used in computer vision, however, they&#39;ve recently been applied on top of pre-trained word vectors for sentence-level classification tasks and the results were promising[5].</p>
<p>Word embeddings can be passed as inputs to CNN models, and cross-validated predictions are obtained from it. These predictions can then be used as new set of features.</p>
<p><img alt="cnn" src="img/8e699b7fc8d1334a.jpg"></p>
<p><a href="http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp" target="_blank">Image Reference</a></p>
<ul>
<li><strong>Recurrent neural networks</strong></li>
</ul>
<p>RNNs like LSTM and GRU are state of the art algorithms for NLP problems. Simply, put a Bi-directional GRU model is putting two independent RNN models in one.</p>
<p>For example, in the sentence &#34;John is walking on the golf court&#34;, a unidirectional model would represent states with representing &#34;golf&#34; based on &#34;John is walking&#34; but not the &#34;court&#34;. Using a bi-directional model, the representation would also account the later representations giving the model more predictive power.</p>
<p>This makes it a more ‘natural&#39; approach when dealing with textual data since the text is naturally sequential[6].</p>
<p><img alt="rnn" src="img/219690c6f1d7b32b.jpg"></p>
<p><a href="https://richliao.github.io/supervised/classification/2016/12/26/textclassifier-RNN" target="_blank">Image Reference</a></p>
<ul>
<li><strong>Transformer based language models</strong></li>
</ul>
<p>Transformer based language models like BERT are state-of-the-art NLP models that can be used for a wide variety of NLP tasks. These models capture the contextual relation between words by using an attention mechanism. Unlike directional models that read text sequentially, a Transformer-based model reads the entire sequence of text at once, allowing it to learn the context of the word based on all of its surrounding words. The embeddings obtained by these models show improved results in comparison to earlier embedding approaches.</p>
<p><img alt="bert-arch" src="img/c658ca69e7464e68.jpg"></p>
<p><a href="https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html" target="_blank">Image Reference</a></p>
<h2>Building Text Classification Models</h2>
<p>Once the features have been extracted, they can then be used for training a classifier.</p>
<h2>References</h2>
<p>[1] <a href="https://stackedit.io/%5Bhttps://en.wikipedia.org/wiki/Natural_language_processing%5D(https://en.wikipedia.org/wiki/Natural_language_processing)" target="_blank">Natural language processing - WIkipedia</a></p>
<p>[2] <a href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf" target="_blank">TF-IDF</a></p>
<p>[3] <a href="https://en.wikipedia.org/wiki/Singular_value_decomposition" target="_blank">SVD</a></p>
<p>[4] <a href="https://code.google.com/archive/p/word2vec/" target="_blank">Word2vec Model</a></p>
<p>[5] <a href="http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/" target="_blank">Understanding Convolutional Neural Networks for NLP</a></p>
<p>[6] <a href="https://arxiv.org/pdf/1408.5882.pdf" target="_blank">Convolutional Neural Networks for Sentence Classification</a></p>
<p>[7] <a href="https://richliao.github.io/supervised/classification/2016/12/26/textclassifier-RNN/" target="_blank">Text Classification, Part 2 - sentence-level Attentional RNN</a></p>
<p>[8] <a href="https://arxiv.org/abs/1706.03762" target="_blank">Attention Is All You Need</a></p>
<p>[9] <a href="https://arxiv.org/abs/1810.04805" target="_blank">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></p>
<h2>Deeper Dive and Resources</h2>
<ul>
<li><a href="https://stackedit.io/%5Bhttps://web.stanford.edu/~jurafsky/slp3/ed3book.pdf%5D(https://web.stanford.edu/~jurafsky/slp3/ed3book.pdf)" target="_blank">Jurafsky, Daniel and James Martin (2008) Speech and Language Processing (Second Edition). Prentice Hall</a></li>
<li><a href="https://www.nltk.org/book/" target="_blank">Natural Language Processing with Python : Steven Bird, Ewan Klein, and Edward Loper</a></li>
</ul>


      </google-codelab-step>
    
      <google-codelab-step label="Task 4: Driverless AI NLP Recipe" duration="0">
        <p>Text data can contain critical information to inform better predictions. H2O Driverless AI automatically converts text strings into features using powerful techniques like TFIDF, CNN, and GRU. Driverless AI version 1.9 introduces support for PyTorch Transformer Architectures (for example, BERT) that can be used for Feature Engineering or as Modeling Algorithms. The Driverless AI platform has the ability to support both standalone text and text with other columns as predictive features. In particular, the following NLP recipes are available for a given text column:</p>
<p><img alt="driverless-nlp-recipe" src="img/46a2a23e473e6343.jpg"></p>
<h2>Key Capabilities of Driverless AI NLP Recipe</h2>
<ul>
<li><strong>n-grams</strong></li>
</ul>
<p>An n-gram is a continuous sequence of n items from a given sample of text or speech.</p>
<ul>
<li><strong>TFIDF of n-grams</strong></li>
</ul>
<p>Frequency-based features are multiplied with inverse document frequency to get TFIDF vectors.</p>
<ul>
<li><strong>Frequency of n-grams</strong></li>
</ul>
<p>Frequency-based features represent the count of each word in the given text in the form of vectors. Frequency-based features are created for different n-gram values[2]. The dimensions of the output vectors are quite high. Words/n-grams that occur more number of times will get higher weightage than the ones that occur less frequently.</p>
<ul>
<li><strong>Truncated SVD Features</strong></li>
</ul>
<p>Both TFIDF and Frequency of n-grams result in a higher dimension. To tackle this, we use Truncated SVD to decompose the vector arrays in lower dimensions.</p>
<ul>
<li><strong>Linear models on TF/IDF vectors</strong></li>
</ul>
<p>In our NLP recipe, we also have linear models on top of n-gram TFIDF / frequency vectors. This capture linear dependencies that are simple yet significant in achieving the best accuracies.</p>
<ul>
<li><strong>Word Embeddings</strong></li>
</ul>
<p>Driverless AI NLP recipe makes use of the power of word embeddings where words or phrases from the vocabulary are mapped to vectors of real numbers.</p>
<ul>
<li><strong>Bi-direction GRU models on Word Embeddings</strong></li>
</ul>
<p>A Bi-directional GRU model is like putting two independent RNN models in one. Taking note of accuracy as well as speed in our experiments, we have decided to take advantage of high speed and almost similar accuracies of GRU architecture compared to its counterpart LSTM.</p>
<ul>
<li><strong>Convolution neural network models on:</strong><ul>
<li><strong>Word Embeddings</strong></li>
</ul>
In Driverless AI, we pass word embeddings as input to CNN models, get cross-validated predictions from it and use them as a new set of features.<ul>
<li><strong>Character embeddings</strong></li>
</ul>
Natural language processing is complex as the language is hard to understand given small data and different languages. Targeting languages like Japanese, Chinese where characters play a major role, we have character level embeddings in our recipe as well.In character embeddings, each character gets represented in the form of vectors rather than words. Driverless AI uses character level embeddings as input to CNN models and later extracts class probabilities to feed as features for downstream models: this gives the ability to work in languages other than English. In languages like Japanese and Chinese, where there is no concept of words, character embeddings will be useful.</li>
<li><strong>BERT/DistilBERT Models for Feature Engineering:</strong></li>
</ul>
<p>BERT and <a href="https://arxiv.org/abs/1910.01108" target="_blank">DistilBERT</a> models can be used for generating embeddings for any text columns. These pre-trained models are used to get embeddings for the text followed by Linear/Logistic Regression to generate features that can then be used for any downstream models in Driverless AI.</p>
<ul>
<li><strong>PyTorch Transformer Architecture Models (eg. BERT) as Modeling Algorithms:</strong></li>
</ul>
<p>Starting with Driverless AI 1.9 release, the Transformer-based architectures shown in the diagram below are supported as models in Driverless AI.</p>
<p><img alt="bert-models" src="img/9336607e1eba5b0d.jpg"></p>
<p>The BERT model supports multiple languages. <a href="https://arxiv.org/abs/1910.01108" target="_blank">DistilBERT</a> is a distilled version of BERT that has fewer parameters compared to BERT (40% less) and it is faster (60% speedup) while retaining 95% of BERT level performance. The DistilBERT model can be useful when training time and model size is important</p>
<ul>
<li><strong>Domain Specific BERT Recipes</strong></li>
</ul>
<p>DAI Base BERT model can also be extended for domain specific problems</p>
<ul>
<li><a href="https://github.com/ProsusAI/finBERT" target="_blank">FinBERT</a> (trained on financial text)</li>
<li><a href="https://github.com/allenai/scibert" target="_blank">SciBERT</a> (trained on scientific text)</li>
<li><a href="https://github.com/dmis-lab/biobert" target="_blank">BioBERT</a> (trained on bio-medical text)</li>
</ul>
<h2>Deeper Dive and Resources</h2>
<ul>
<li><a href="https://www.youtube.com/watch?v=8bumqmOfIYs&t=1488s" target="_blank">Text Classification with H2O Driverless AI - A Look Under the Hood</a></li>
<li><a href="https://www.h2o.ai/webinars/?commid=428217" target="_blank">State of The Art NLP Models in H2O Driverless AI 1.9</a></li>
</ul>
<h2>Industry Use Cases leveraging NLP</h2>
<ul>
<li><a href="https://www.youtube.com/watch?v=D0e-1gedK6g&feature=youtu.be" target="_blank">ArmadaHealth: Understanding Sentiment to impact lives</a></li>
<li><a href="https://youtu.be/aXPE6IiKRmI" target="_blank">The AI Academy is accelerating NLP projects with Driverless AI</a></li>
<li><a href="https://www.youtube.com/watch?v=f4b2Yoe9JEs&feature=youtu.be" target="_blank">Martin Stein, G5 - Driving Marketing Performance with H2O Driverless AI</a></li>
</ul>


      </google-codelab-step>
    
      <google-codelab-step label="Task 5: Experiment Results Summary" duration="0">
        <p>After an experiment status changes from <code>RUNNING</code> to <code>COMPLETE</code>, the UI provides you with several options:</p>
<p><img alt="experiment-results-ui" src="img/45eb0261bbac7641.jpg"></p>
<p>The Experiment Summary contains a lot of useful information which helps to understand what goes under the hood during the Sentiment Analysis Experiment. If you are interested in learning more about each plot and the metrics derived from those plots covered in this section, then check out this tutorial <a href="https://training.h2o.ai/products/tutorial-1b-machine-learning-experiment-scoring-and-analysis-tutorial-financial-focus" target="_blank">Machine Learning Experiment Scoring and Analysis Tutorial - Financial Focus</a>.</p>
<p>Please note that the lab also provides a pre-ran experiment. You can either wait for your experiments to finish or use the results of the pre-ran experiment. Click on the <strong>Experiments</strong> tab and select the Amazon Fine Food reviews experiment as follows:</p>
<p><img alt="pre-ran-experiment" src="img/f5f33bf40ae5b318.jpg"></p>
<p><em>Things to Note:</em></p>
<ol type="1">
<li>Status Complete Options<ul>
<li><strong>Deploy:</strong> Whether to deploy the Model to Cloud or keep it local.</li>
<li><strong>Interpret this Model</strong> - Refers to  Interpreting a machine learning model  in a human-readable format.Click on <code>Intrepret this Model</code> button on the completed experiment page to interpret the  Driverless AI NLP model on original and transformed features. This launches the Model Interpretation for that experiment. This page provides several visual explanations and reason codes for the trained  model and its results. Here we shall only go through the explanation of the NLP specific models. For the rest,refer to the tutorial-<a href="https://training.h2o.ai/products/tutorial-1b-machine-learning-experiment-scoring-and-analysis-tutorial-financial-focus" target="_blank">Machine Learning Experiment Scoring and Analysis Tutorial - Financial Focus</a>.<ul>
<li><strong>NLP TOKENS</strong></li>
</ul>
This plot is available for natural language processing (NLP) models. It is located in the Dataset tab on the Model Interpretation page (only visible  for NLP models). It shows both the global and local importance values of each token in a corpus (a large and structured set of texts). The corpus is automatically generated from text features used by Driverless AI models prior to the process of tokenization.<ul>
<li><strong>NLP LOCO</strong></li>
</ul>
This plot is available for natural language processing (NLP) models. It applies a leave-one-covariate-out (LOCO) styled approach to NLP models by removing a specific token from all text features in a record and predicting local importance without that token. The difference between the resulting score and the original score (token included) is useful when trying to determine how specific changes to text features alter the predictions made by the model.</li>
<li><strong>Diagnose Model on New Dataset</strong>  - Allows you to view model performance for multiple scorers based on existing model and a test dataset</li>
<li><strong>Score on another Dataset</strong>  - After you generate a model, you can use that model to make predictions on another dataset.</li>
<li><strong>Transform Another Dataset</strong> - One can specify to transform the new dataset after adding it to Driverless AI, and the same transformations that Driverless AI applied to the original dataset will be applied to these new rows.</li>
<li><strong>Download Predictions</strong><ul>
<li><strong>Training (Holdout) Predictions</strong> : In csv format, available if a validation set was NOT provided.</li>
<li><strong>Validation Set Predictions</strong> : In csv format, available if a validation set was provided.</li>
<li><strong>Test Set Predictions</strong> : In csv format, available if a test dataset is used.</li>
</ul>
</li>
<li><strong>Build Python Scoring Pipeline</strong>  - A standalone Python scoring pipeline for H2O Driverless AI</li>
<li><strong>Build MOJO Scoring Pipeline</strong> - A standalone Model ObJect, Optimized scoring pipeline that can be easily embeddable in java environment&#39;</li>
<li><strong>Visualize Scoring Pipeline(Experimental)</strong> - Opens an experiment pipeline visualization page. Refer to <a href="http://docs.h2o.ai/driverless-ai/latest-stable/docs/userguide/scoring_pipeline_visualize.html#visualize-scoring-pipeline" target="_blank">Visualizing the Scoring Pipeline</a>.</li>
<li><strong>Download Summary and Logs</strong>  - An experiment summary is available for each completed experiment as zip file.</li>
<li><strong>Download AutoReport</strong> - A Word version of an auto-generated report for the experiment. This file is also available in the Experiment Summary zip file.</li>
</ul>
</li>
<li>Iteration Data - Validation<ul>
<li>The winning model&#39;s validation score and the algorithm used are as follows:<br><br><ul>
<li>Validation Score - .2207</li>
<li>Model Type: TEXTBERT</li>
</ul>
</li>
</ul>
</li>
<li>Variable Importance: Summary of top 20 - Feature Engineered variablesDriverless AI performs feature Engineering on the dataset to determine the optimal representation of the data. Various stages of the features appear throughout the iteration of the data. These can be viewed by hovering over points on the Iteration Data - Validation Graph and seeing the updates on the  <strong>Variable Importance</strong> section.<img alt="variable-importance" src="img/b1d52cf843f1a862.jpg">The complete list of features used in the final model is available in the Experiment Summary artifacts. The Experiment Summary also provides a list of the original features and their estimated feature importance.</li>
<li>SummaryThis option gives a brief summary of the entire experiment including :<ul>
<li>How many features were tested and selected?</li>
<li>How many models were trained for feature evolution?</li>
</ul>
</li>
</ol>
<p>There are also several plots adjacent to the summary tab that give insight into the experiment. If you are interested in learning more about each plot and the metrics derived from those plots covered in this section, then check out our next tutorial <a href="https://training.h2o.ai/products/tutorial-1b-machine-learning-experiment-scoring-and-analysis-tutorial-financial-focus" target="_blank">Machine Learning Experiment Scoring and Analysis Tutorial - Financial Focus</a>.</p>
<h2>Deeper Dive and Resources</h2>
<ul>
<li><a href="http://docs.h2o.ai/driverless-ai/latest-stable/docs/userguide/running-experiment.html#" target="_blank">Learn more about running Experiments from H2O Driverless AI docs</a></li>
<li><a href="http://docs.h2o.ai/driverless-ai/latest-stable/docs/userguide/experiment-completed.html" target="_blank">Explore Documentation on Completed Experiments</a></li>
<li><a href="http://docs.h2o.ai/driverless-ai/latest-stable/docs/userguide/experiment-summary.html" target="_blank">Explore Documentation on Experiment Summary</a></li>
<li><a href="http://docs.h2o.ai/driverless-ai/latest-stable/docs/booklets/DriverlessAIBooklet.pdf" target="_blank">Review the Driverless AI Booklet to learn more about running experiments</a></li>
<li><a href="http://docs.h2o.ai/driverless-ai/latest-stable/docs/userguide/transformations.html" target="_blank">Learn more about Driverless AI Transformations</a></li>
<li><a href="https://www.youtube.com/playlist?list=PLrsf4weWJKynQBvh0i-YxDDVqCcIrF28o" target="_blank">Feature Engineering for ML by Dmitry Lark</a></li>
</ul>


      </google-codelab-step>
    
      <google-codelab-step label="Task 6: Custom Recipe to Improve Predictions" duration="0">
        <p>The latest version(1.9.0) of Driverless AI implements a key feature called BYOR[1], which stands for Bring Your Own Recipes and was introduced in with Driverless AI (1.7.0). This feature has been designed to enable Data Scientists or domain experts to influence and customize the machine learning optimization used by Driverless AI as per their business needs. This additional feature engineering technique is aimed towards improving the accuracy of the model.</p>
<p>Recipes are customizations and extensions to the Driverless AI platform. TThey are nothing but Python code snippets uploaded into Driverless AI at runtime, like plugins. Recipes can be either one or a combination of the following:</p>
<ul>
<li>Custom machine learning models</li>
<li>Custom scorers (classification or regression)</li>
<li>Custom transformers<br></li>
</ul>
<p><img alt="recipes-workflow" src="img/deb58a83fc2b5888.jpg"></p>
<h2>Uploading a Custom Recipe</h2>
<p>H2O has built and open-sourced several recipes[2] which can be used as templates. For this experiment, we shall use the following recipe: <a href="https://github.com/h2oai/driverlessai-recipes/blob/master/transformers/nlp/text_sentiment_transformer.py" target="_blank">text_sentiment_transformer.py</a> which extracts sentiment from text using pre-trained models from TextBlob[3].</p>
<p>1. Start a new Driverless AI experiment as explained in Task 1 and Task 2 and click on the Expert Settings.</p>
<p><img alt="clickon-expert-settings" src="img/caf7d994db39aaa6.jpg"></p>
<p>2. A new window with Expert Experiment Settings pops up. Here you can either upload a custom recipe or load custom recipe from url.</p>
<p><img alt="expert-experiment-settings-overview" src="img/850edbeb346eec19.jpg"></p>
<p>3. To upload a custom recipe, Click on the UPLOAD CUSTOM RECIPE tab and select the desired recipe. Click save when done.</p>
<p>4. Alternately, you can also upload a recipe via URL. Click on the LOAD CUSTOM RECIPE FROM URL tab and enter the raw Github URL of the recipe. Click save when done</p>
<pre><code>https://github.com/h2oai/driverlessai-recipes/blob/rel-1.9.0/transformers/nlp/text_sentiment_transformer.py
</code></pre>
<p><img alt="uploading-recipe-from-url" src="img/3fcb15d59f3d295d.jpg"></p>
<p>5. You&#39;re welcome to create your own recipes, or you can select from a number of available recipes.  The OFFICIAL RECIPES(EXTERNAL) tab will directly take you to the recipes compatible with the Driverless AI version that you are using.</p>
<p><img alt="official-recipes" src="img/256d228c321c8da3.jpg"></p>
<p>6. Once the recipe is uploaded, the following screen will appear. Driverless AI automatically performs basic acceptance tests for all custom recipes unless disabled.</p>
<p><img alt="acceptance-tests" src="img/976375f4ac333ded.jpg"></p>
<p>7. Click on Recipe and select or deselect specific transformers, models and scorers.</p>
<p><img alt="selecting-specific-transformers" src="img/1d20494fa614712d.jpg"></p>
<p><em>Things to Note</em></p>
<ol type="1">
<li>Select specific Transformers</li>
<li>Select specific Models</li>
<li>Select specific Scorers</li>
<li>Include specific preprocessing Transformers</li>
<li>Include specific data recipes during experiment</li>
<li>Scorer to optimize threshold</li>
<li>Enable if you want to skip failure of transformers</li>
<li>Enable if you want to skip failure of models</li>
</ol>
<p>8. Click Save to save the settings. The selected transformer should now appear on the main Experiment screen as follows.</p>
<p><img alt="main-screen-after-expert-settings" src="img/9bcfb9d655c7fa.jpg"></p>
<p>9. Launch the Experiment with the Custom Recipe.</p>
<h2>References</h2>
<p>[1] <a href="https://www.h2o.ai/blog/custom-machine-learning-recipes-the-ingredients-for-success/" target="_blank">Custom Machine Learning Recipes: The ingredients for success</a></p>
<p>[2] <a href="https://github.com/h2oai/driverlessai-recipes" target="_blank">Driverless AI Recipes</a></p>
<p>[3] <a href="https://textblob.readthedocs.io/en/dev/" target="_blank">TextBlob</a></p>
<h2>Deeper Dive and Resources</h2>
<ul>
<li><a href="https://h2oai.github.io/tutorials/get-started-and-consume-existing-recipes" target="_blank">Get Started and Consume Existing Recipes</a></li>
</ul>


      </google-codelab-step>
    
      <google-codelab-step label="Task 7: Challenge" duration="0">
        <p>It&#39;s time to test your skills!</p>
<p>The challenge is to analyze and perform Sentiment Analysis on the tweets using the US Airline Sentiment dataset. This will help to gauge people&#39;s sentiments about each of the major U.S. airline.</p>
<p>This data comes from  <a href="http://www.crowdflower.com/data-for-everyone" target="_blank">Crowdflower&#39;s Data for Everyone library</a>  and constitutes twitter reviews about how travellers in February 2015 expressed their feelings on Twitter about every major U.S. airline. The reviews have been classified as positive, negative, and neutral.</p>
<h2>Steps:</h2>
<p>1. Import the dataset from here:</p>
<ul>
<li><a href="https://data.world/crowdflower/airline-twitter-sentiment" target="_blank">Airline-Sentiment-2-w-AA</a></li>
</ul>
<p>Here are some samples from the dataset:</p>
<p><img alt="challenge-dataset" src="img/7595f43924eb8589.jpg"></p>
<p>2. Split the dataset into a training set and a testing set in an 80:20 ratio.</p>
<p>3. Run an experiment where the target column is &#34;<strong>airline_sentiment</strong>&#34; using only the default Transformers. You can exclude all other columns from the dataset except the ‘text&#39; column.</p>
<p>4. Run another instance of the same experiment but this time also include Tensorflow models in addition to the built in transformers.</p>
<p>5. Next, repeat the experiment with a custom recipe from <a href="https://github.com/h2oai/driverlessai-recipes/blob/master/README.md#sample-recipes" target="_blank">here</a>.</p>
<p>6. Using Logloss as the scorer, observe the following outcomes:</p>
<ul>
<li>Which experiment out of the three gives the minimum Logloss value and why?</li>
<li>How variable importance changes as you change the selected transformers.</li>
</ul>
<h2>Deeper Dive and Resources</h2>
<ul>
<li><a href="https://github.com/h2oai/driverlessai-tutorials/blob/master/driverlessai_experiments/nlp/airline_sentiment_experiment/demo_nlp_airline_sentiment.ipynb" target="_blank">Try running an experiment without the Driverless AI UI using the Python Client</a></li>
</ul>


      </google-codelab-step>
    
      <google-codelab-step label="Task 8: Appendix A: Add the Datasets" duration="0">
        <h2>Add the Datasets</h2>
<p>Import Amazon Fine Food Reviews training and test datasets to the Datasets Overview Page.</p>
<p>1. Select +Add Dataset(or Drag and Drop) then click on File System</p>
<p><img alt="appendix-add-datasets" src="img/4cc5bf254f5de1c.jpg"></p>
<p>2. Enter the following : <code>data/Kaggle/AmazonFineFoodReviews/</code> into the search bar</p>
<p>3. Select AmazonFineFoodReviews&#39; training and test datasets.</p>
<p><img alt="appendix-datasets-preview" src="img/39d0b205b776817a.jpg"></p>
<p>4. Click to Import Selection</p>
<p>5. If the file loaded successfully then the following image should appear on the screen</p>
<p><img alt="appendix-upload-dataset" src="img/3121a89a90ddd257.jpg"></p>
<p>You can see that a new set of datasets now appear on the screen.</p>


      </google-codelab-step>
    
      <google-codelab-step label="Next Steps" duration="0">
        <p>Check out the <a href="https://training.h2o.ai/products/tutorial-3a-get-started-with-open-source-custom-recipes-tutorial" target="_blank">Get Started with Open Source Custom Recipes</a> tutorial, where you will learn:</p>
<ul>
<li>What is Bring Your Own Recipe</li>
<li>What is a recipe</li>
<li>Types of Driverless AI open-source recipes available</li>
<li>How to upload the recipes into Driverless AI as raw URL&#39;s and .py files</li>
<li>Compared the final models of the experiments that were run with custom recipes to Driverless AI&#39;s default settings experiment</li>
</ul>


      </google-codelab-step>
    
  </google-codelab>

  <script src="../assets/codelab-elements/native-shim.js"></script>
  <script src="../assets/codelab-elements/custom-elements.min.js"></script>
  <script src="../assets/codelab-elements/prettify.js"></script>
  <script src="../assets/codelab-elements/codelab-elements.js"></script>

</body>
</html>
