{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Driverless AI: Using the Python API\n",
    "This notebook provides an H2OAI Client workflow, of model building and scoring, that parallels the Driverless AI workflow.\n",
    "\n",
    "## Notes:\n",
    "\n",
    "* This is an early release of the Driverless AI Python client.\n",
    "* This notebook is specifically for the Driverless AI 1.8.X series.\n",
    "* Python 3.6 is the only supported version.\n",
    "* You must install the `h2oai_client` wheel to your local Python. This is available from the RESOURCES link in the top menu of the UI.\n",
    "\n",
    "![py-client](images/py_client_link.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow Steps\n",
    "\n",
    "**Build an Experiment with Python API:**\n",
    "\n",
    "1. Sign in\n",
    "2. Import train & test set/new data\n",
    "3. Specify experiment parameters\n",
    "4. Launch Experiement\n",
    "5. Examine Experiment\n",
    "6. Download Predictions\n",
    "    \n",
    "**Build an Experiment in Web UI and Access Through Python:**\n",
    "\n",
    "1. Get pointer to experiment\n",
    "    \n",
    "**Score on New Data:**\n",
    "\n",
    "1. Score on new data with H2OAI model\n",
    "\n",
    "**Model Diagnostics on New Data:**\n",
    "\n",
    "1. Run model diagnostincs on new data with H2OAI model\n",
    "\n",
    "**Run Model Interpretation**\n",
    "\n",
    "1. Run model interpretation on the raw features\n",
    "2. Run Model Interpretation on External Model Predictions\n",
    "\n",
    "**Build Scoring Pipelines**\n",
    "\n",
    "1. Build Python Scoring Pipeline\n",
    "2. Build MOJO Scoring Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build an Experiment with Python API\n",
    "\n",
    "### 1. Sign In\n",
    "\n",
    "Import the required modules and log in.\n",
    "\n",
    "Pass in your credentials through the Client class which creates an authentication token to send to the Driverless AI Server. In plain English: to sign into the Driverless AI web page (which then sends requests to the Driverless Server), instantiate the Client class with your Driverless AI address and login credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h2oai_client\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import h2o\n",
    "import requests\n",
    "import math\n",
    "from h2oai_client import Client, ModelParameters, InterpretParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# address = 'http://ip_where_driverless_is_running:12345'\n",
    "# username = 'username'\n",
    "# password = 'password'\n",
    "# h2oai = Client(address = address, username = username, password = password)\n",
    "# make sure to use the same user name and password when signing in through the GUI\n",
    "h2oai = Client(\"http://34.207.226.81:12345\", \"h2oai\", \"i-0c12a59b8c0ab004c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Equivalent Steps in Driverless: Signing In\n",
    "![Equivalent Steps in Driverless: Signing In](images/sign_in_home_page_0.png)\n",
    "\n",
    "![Equivalent Steps in Driverless: Signing In](images/skip_sign_in_home_page_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Upload Datasets\n",
    "\n",
    "Upload training and testing datasets from the Driverless AI **/data** folder.\n",
    "\n",
    "You can provide a training, validation, and testing dataset for an experiment.  The validation and testing dataset are optional.  In this example, we will provide only training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '/data/Kaggle/CreditCard/CreditCard-train.csv'\n",
    "test_path = '/data/Kaggle/CreditCard/CreditCard-test.csv'\n",
    "\n",
    "train = h2oai.create_dataset_sync(train_path)\n",
    "test = h2oai.create_dataset_sync(test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Equivalent Steps in Driverless: Uploading Train & Test CSV Files\n",
    "![Equivalent Steps in Driverless: Upload Train & Test CSV Files](images/import_data_sets_creditcard.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3. Set Experiment Parameters\n",
    "\n",
    "We will now set the parameters of our experiment.  Some of the parameters include:\n",
    "\n",
    "* Target Column: The column we are trying to predict.\n",
    "* Dropped Columns: The columns we do not want to use as predictors such as ID columns, columns with data leakage, etc.\n",
    "* Weight Column: The column that indicates the per row observation weights. If `None`, each row will have an observation weight of 1.\n",
    "* Fold Column: The column that indicates the fold. If `None`, the folds will be determined by Driverless AI.\n",
    "* Is Time Series: Whether or not the experiment is a time-series use case.\n",
    "\n",
    "For information on the experiment settings, refer to the [Experiment Settings](http://docs.h2o.ai/driverless-ai/latest-stable/docs/userguide/running-experiment.html#experiment-settings).\n",
    "\n",
    "For this example, we will be predicting **`default payment next month`**.  The parameters that control the experiment process are: `accuracy`, `time`, and `interpretability`.  We can use the `get_experiment_preview_sync` function to get a sense of what will happen during the experiment.  \n",
    "\n",
    "We will start out by seeing what the experiment will look like with `accuracy`, `time`, and `interpretability` all set to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ACCURACY [5/10]:',\n",
       " '- Training data size: *23,999 rows, 25 cols*',\n",
       " '- Feature evolution: *[LightGBM, XGBoostGBM]*, *1/3 validation split*',\n",
       " '- Final pipeline: *Ensemble (4 models), 4-fold CV*',\n",
       " '',\n",
       " 'TIME [5/10]:',\n",
       " '- Feature evolution: *4 individuals*, up to *66 iterations*',\n",
       " '- Early stopping: After *10* iterations of no improvement',\n",
       " '',\n",
       " 'INTERPRETABILITY [5/10]:',\n",
       " '- Feature pre-pruning strategy: None',\n",
       " '- Monotonicity constraints: disabled',\n",
       " '- Feature engineering search space (where applicable): [CVCatNumEncode, CVTargetEncode, ClusterDist, ClusterId, ClusterTE, Dates, Frequent, Interactions, IsHoliday, NumCatTE, NumToCatTE, NumToCatWoE, Original, TextLinModel, Text, TruncSVDNum, WeightOfEvidence]',\n",
       " '',\n",
       " '[LightGBM, XGBoostGBM] models to train:',\n",
       " '- Model and feature tuning: *16*',\n",
       " '- Feature evolution: *104*',\n",
       " '- Final pipeline: *4*',\n",
       " '',\n",
       " 'Estimated runtime: *minutes*',\n",
       " 'Auto-click Finish if not done in: *1 day*']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target=\"default payment next month\"\n",
    "exp_preview = h2oai.get_experiment_preview_sync(dataset_key= train.key\n",
    "                                                , validset_key=''\n",
    "                                                , classification=True\n",
    "                                                , dropped_cols = []\n",
    "                                                , target_col=target\n",
    "                                                , is_time_series = False\n",
    "                                                , enable_gpus = True\n",
    "                                                , accuracy = 5, time = 5, interpretability = 5\n",
    "                                                , reproducible = True\n",
    "                                                , resumed_experiment_id = \"\" # This is a new experiment\n",
    "                                                , config_overrides = None)\n",
    "exp_preview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these settings, the Driverless AI experiment will train about 124 models: \n",
    "* 16 for model and feature tuning\n",
    "* 104 for feature evolution\n",
    "* 4 for the final pipeline\n",
    "\n",
    "When we start the experiment, we can either: \n",
    "\n",
    "* specify parameters\n",
    "* use Driverless AI to suggest parameters\n",
    "\n",
    "Driverless AI can suggest the parameters based on the dataset and target column.  Below we will use the **`get_experiment_tuning_suggestion`** to see what settings Driverless AI suggests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let Driverless suggest parameters for experiment\n",
    "params = h2oai.get_experiment_tuning_suggestion(dataset_key = train.key\n",
    "                                                , target_col = target\n",
    "                                                , is_classification = True\n",
    "                                                , is_time_series = False\n",
    "                                                , config_overrides = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset': {'key': '477016ec-f7fd-11e9-ab33-0242ac110002',\n",
       "  'display_name': ''},\n",
       " 'resumed_model': {'key': '', 'display_name': ''},\n",
       " 'target_col': 'default payment next month',\n",
       " 'weight_col': '',\n",
       " 'fold_col': '',\n",
       " 'orig_time_col': '',\n",
       " 'time_col': '',\n",
       " 'is_classification': True,\n",
       " 'cols_to_drop': [],\n",
       " 'validset': {'key': '', 'display_name': ''},\n",
       " 'testset': {'key': '', 'display_name': ''},\n",
       " 'enable_gpus': True,\n",
       " 'seed': False,\n",
       " 'accuracy': 5,\n",
       " 'time': 4,\n",
       " 'interpretability': 6,\n",
       " 'score_f_name': 'AUC',\n",
       " 'time_groups_columns': [],\n",
       " 'time_period_in_seconds': None,\n",
       " 'num_prediction_periods': None,\n",
       " 'num_gap_periods': None,\n",
       " 'is_timeseries': False,\n",
       " 'config_overrides': None}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params.dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Driverless AI has found that the best parameters are to set **`accuracy = 5`**, **`time = 4`**, **`interpretability = 6`**. It has selected **`AUC`** as the scorer (this is the default scorer for binomial problems).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Equivalent Steps in Driverless: Set the Knobs, Configuration & Launch\n",
    "![Equivalent Steps in Driverless: Set the Knobs](images/set_parameters_creditcard.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Launch Experiment: Feature Engineering + Final Model Training\n",
    "\n",
    "Launch the experiment using the parameters that Driverless AI suggested along with the testset, scorer, and seed that were added. We can launch the experiment with the suggested parameters or create our own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = h2oai.start_experiment_sync(dataset_key=train.key,\n",
    "                                         testset_key = test.key,\n",
    "                                         target_col=target,\n",
    "                                         is_classification=True,\n",
    "                                         accuracy=5,\n",
    "                                         time=4,\n",
    "                                         interpretability=6,\n",
    "                                         scorer=\"AUC\",\n",
    "                                         enable_gpus=True,\n",
    "                                         seed=1234,\n",
    "                                         cols_to_drop=['ID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Equivalent Steps in Driverless: Launch Experiment\n",
    "\n",
    "![Equivalent Steps in Driverless: Launch Your Experiment](images/exp_running_creditcard.png)\n",
    "\n",
    "![Equivalent Steps in Driverless: Launch Your Experiment](images/experiment_list_running.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Examine Experiment\n",
    "\n",
    "View the final model score for the validation and test datasets. When feature engineering is complete, an ensemble model can be built depending on the accuracy setting. The experiment object also contains the score on the validation and test data for this ensemble model.  In this case, the validation score is the score on the training cross-validation predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Model Score on Validation Data: 0.779\n",
      "Final Model Score on Test Data: 0.8\n"
     ]
    }
   ],
   "source": [
    "print(\"Final Model Score on Validation Data: \" + str(round(experiment.valid_score, 3)))\n",
    "print(\"Final Model Score on Test Data: \" + str(round(experiment.test_score, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The experiment object also contains the scores calculated for each iteration on bootstrapped samples on the validation data.  In the iteration graph in the UI, we can see the mean performance for the best model (yellow dot) and +/- 1 standard deviation of the best model performance (yellow bar).\n",
    "\n",
    "This information is saved in the experiment object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Add scores from experiment iterations\n",
    "iteration_data = h2oai.list_model_iteration_data(experiment.key, 0, len(experiment.iteration_data))\n",
    "iterations = list(map(lambda iteration: iteration.iteration, iteration_data))\n",
    "scores_mean = list(map(lambda iteration: iteration.score_mean, iteration_data))\n",
    "scores_sd = list(map(lambda iteration: iteration.score_sd, iteration_data))\n",
    "\n",
    "# Add score from final ensemble\n",
    "iterations = iterations + [max(iterations) + 1]\n",
    "scores_mean = scores_mean + [experiment.valid_score]\n",
    "scores_sd = scores_sd + [experiment.valid_score_sd]\n",
    "\n",
    "plt.figure()\n",
    "plt.errorbar(iterations, scores_mean, yerr=scores_sd, color = \"y\", \n",
    "             ecolor='yellow', fmt = '--o', elinewidth = 4, alpha = 0.5)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"AUC\")\n",
    "plt.ylim([0.75, 0.82])\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Equivalent Steps in Driverless: View Results\n",
    "![Equivalent Steps in Driverless: View Results](images/experiment_complete_creditcard.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Download Results\n",
    "\n",
    "Once an experiment is complete, we can see that the UI presents us options of downloading the: \n",
    "\n",
    "* predictions \n",
    "    * on the (holdout) train data\n",
    "    * on the test data\n",
    "* experiment summary - summary of the experiment including feature importance\n",
    "\n",
    "We will show an example of downloading the test predictions below.  Note that equivalent commands can also be run for downloading the train (holdout) predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./test_preds.csv'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h2oai.download(src_path=experiment.test_predictions_path, dest_dir=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>default payment next month.0</th>\n",
       "      <th>default payment next month.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.457594</td>\n",
       "      <td>0.542405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.856039</td>\n",
       "      <td>0.143961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.935652</td>\n",
       "      <td>0.064348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.567411</td>\n",
       "      <td>0.432589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.862044</td>\n",
       "      <td>0.137956</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   default payment next month.0  default payment next month.1\n",
       "0                      0.457594                      0.542405\n",
       "1                      0.856039                      0.143961\n",
       "2                      0.935652                      0.064348\n",
       "3                      0.567411                      0.432589\n",
       "4                      0.862044                      0.137956"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds = pd.read_csv(\"./test_preds.csv\")\n",
    "test_preds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also download and examine the summary of the experiment and feature importance for the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download Summary\n",
    "import subprocess\n",
    "summary_path = h2oai.download(src_path=experiment.summary_path, dest_dir=\".\")\n",
    "dir_path = \"./h2oai_experiment_summary_\" + experiment.key\n",
    "subprocess.call(['unzip', '-o', summary_path, '-d', dir_path], shell=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table below shows the feature name, its relative importance, and a description.  Some features will be engineered by Driverless AI and some can be the original feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Relative Importance</th>\n",
       "      <th>Feature</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>10_PAY_0</td>\n",
       "      <td>PAY_0 (original)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.55276</td>\n",
       "      <td>11_PAY_2</td>\n",
       "      <td>PAY_2 (original)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.52305</td>\n",
       "      <td>81_InteractionAdd:PAY_0:PAY_2</td>\n",
       "      <td>[PAY_0] + [PAY_2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.49749</td>\n",
       "      <td>54_NumToCatWoE:PAY_0:PAY_3:PAY_5.0</td>\n",
       "      <td>Weight of Evidence for columns ['PAY_0', 'PAY_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.19790</td>\n",
       "      <td>8_LIMIT_BAL</td>\n",
       "      <td>LIMIT_BAL (original)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.19642</td>\n",
       "      <td>1_BILL_AMT1</td>\n",
       "      <td>BILL_AMT1 (original)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.17087</td>\n",
       "      <td>17_PAY_AMT2</td>\n",
       "      <td>PAY_AMT2 (original)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.16198</td>\n",
       "      <td>12_PAY_3</td>\n",
       "      <td>PAY_3 (original)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.15954</td>\n",
       "      <td>16_PAY_AMT1</td>\n",
       "      <td>PAY_AMT1 (original)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.15726</td>\n",
       "      <td>66_ClusterTE:ClusterID76:AGE:PAY_0:PAY_2:PAY_3...</td>\n",
       "      <td>Out-of-fold mean of the response grouped by: [...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Relative Importance                                            Feature  \\\n",
       "0              1.00000                                           10_PAY_0   \n",
       "1              0.55276                                           11_PAY_2   \n",
       "2              0.52305                      81_InteractionAdd:PAY_0:PAY_2   \n",
       "3              0.49749                 54_NumToCatWoE:PAY_0:PAY_3:PAY_5.0   \n",
       "4              0.19790                                        8_LIMIT_BAL   \n",
       "5              0.19642                                        1_BILL_AMT1   \n",
       "6              0.17087                                        17_PAY_AMT2   \n",
       "7              0.16198                                           12_PAY_3   \n",
       "8              0.15954                                        16_PAY_AMT1   \n",
       "9              0.15726  66_ClusterTE:ClusterID76:AGE:PAY_0:PAY_2:PAY_3...   \n",
       "\n",
       "                                         Description  \n",
       "0                                   PAY_0 (original)  \n",
       "1                                   PAY_2 (original)  \n",
       "2                                  [PAY_0] + [PAY_2]  \n",
       "3  Weight of Evidence for columns ['PAY_0', 'PAY_...  \n",
       "4                               LIMIT_BAL (original)  \n",
       "5                               BILL_AMT1 (original)  \n",
       "6                                PAY_AMT2 (original)  \n",
       "7                                   PAY_3 (original)  \n",
       "8                                PAY_AMT1 (original)  \n",
       "9  Out-of-fold mean of the response grouped by: [...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View Features\n",
    "features = pd.read_table(dir_path + \"/ensemble_features.txt\", sep=',', skipinitialspace=True)\n",
    "features.head(n = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build an Experiment in Web UI and Access Through Python\n",
    "\n",
    "It is also possible to use the Python API to examine an experiment that was started through the Web UI using the experiment key.\n",
    "\n",
    "![Experiments List](images/experiment_list_complete.png)\n",
    "\n",
    "### 1. Get pointer to experiment\n",
    "\n",
    "You can get a pointer to the experiment by referencing the experiment key in the Web UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hesenifo', 'hefihuci']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get list of experiments\n",
    "experiment_list = list(map(lambda x: x.key, h2oai.list_models(offset=0, limit=100)))\n",
    "experiment_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get pointer to experiment\n",
    "experiment = h2oai.get_model_job(experiment_list[0]).entity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score on New Data\n",
    "\n",
    "You can use the Python API to score on new data. This is equivalent to the **SCORE ON ANOTHER DATASET** button in the Web UI. The example below scores on the test data and then downloads the predictions.\n",
    "\n",
    "Pass in any dataset that has the same columns as the original training set. If you passed a test set during the H2OAI model building step, the predictions already exist. Its path can be found with **`experiment.test_predictions_path`**.\n",
    "\n",
    "### 1. Score Using the H2OAI Model\n",
    "\n",
    "The following shows the predicted probability of default for each record in the test.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>default payment next month.0</th>\n",
       "      <th>default payment next month.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.395356</td>\n",
       "      <td>0.604644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.857802</td>\n",
       "      <td>0.142198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.943540</td>\n",
       "      <td>0.056460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.533895</td>\n",
       "      <td>0.466105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.875338</td>\n",
       "      <td>0.124662</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   default payment next month.0  default payment next month.1\n",
       "0                      0.395356                      0.604644\n",
       "1                      0.857802                      0.142198\n",
       "2                      0.943540                      0.056460\n",
       "3                      0.533895                      0.466105\n",
       "4                      0.875338                      0.124662"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = h2oai.make_prediction_sync(experiment.key, test_path, output_margin = False, pred_contribs = False)\n",
    "pred_path = h2oai.download(prediction.predictions_csv_path, '.')\n",
    "pred_table = pd.read_csv(pred_path)\n",
    "pred_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get the contribution each feature had to the final prediction by setting `pred_contribs = True`.  This will give us an idea of how each feature effects the predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contrib_0_AGE</th>\n",
       "      <th>contrib_1_BILL_AMT1</th>\n",
       "      <th>contrib_10_PAY_0</th>\n",
       "      <th>contrib_11_PAY_2</th>\n",
       "      <th>contrib_12_PAY_3</th>\n",
       "      <th>contrib_13_PAY_4</th>\n",
       "      <th>contrib_14_PAY_5</th>\n",
       "      <th>contrib_15_PAY_6</th>\n",
       "      <th>contrib_16_PAY_AMT1</th>\n",
       "      <th>contrib_17_PAY_AMT2</th>\n",
       "      <th>...</th>\n",
       "      <th>contrib_71_InteractionDiv:PAY_0:PAY_AMT3</th>\n",
       "      <th>contrib_72_NumToCatTE:BILL_AMT1:PAY_0.0</th>\n",
       "      <th>contrib_73_NumToCatTE:BILL_AMT1:LIMIT_BAL:PAY_5:PAY_AMT2:PAY_AMT4.0</th>\n",
       "      <th>contrib_75_InteractionMul:BILL_AMT1:PAY_AMT2</th>\n",
       "      <th>contrib_76_ClusterTE:ClusterID96:BILL_AMT1:BILL_AMT4:BILL_AMT5:PAY_0:PAY_AMT2:PAY_AMT4.0</th>\n",
       "      <th>contrib_77_ClusterTE:ClusterID51:BILL_AMT1:PAY_0:PAY_3.0</th>\n",
       "      <th>contrib_78_InteractionSub:LIMIT_BAL:PAY_4</th>\n",
       "      <th>contrib_79_ClusterTE:ClusterID54:BILL_AMT1:PAY_3:PAY_AMT2:PAY_AMT6.0</th>\n",
       "      <th>contrib_80_NumToCatTE:PAY_0:PAY_AMT2.0</th>\n",
       "      <th>contrib_bias</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.010583</td>\n",
       "      <td>-0.055527</td>\n",
       "      <td>0.573270</td>\n",
       "      <td>0.153810</td>\n",
       "      <td>-0.001514</td>\n",
       "      <td>0.000183</td>\n",
       "      <td>-0.007137</td>\n",
       "      <td>-0.019053</td>\n",
       "      <td>0.035922</td>\n",
       "      <td>-0.002453</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001213</td>\n",
       "      <td>0.078841</td>\n",
       "      <td>-0.005750</td>\n",
       "      <td>-0.002988</td>\n",
       "      <td>0.066117</td>\n",
       "      <td>0.050699</td>\n",
       "      <td>0.023325</td>\n",
       "      <td>-0.004212</td>\n",
       "      <td>0.001306</td>\n",
       "      <td>-1.427382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.020959</td>\n",
       "      <td>-0.065462</td>\n",
       "      <td>-0.105277</td>\n",
       "      <td>-0.025094</td>\n",
       "      <td>-0.016801</td>\n",
       "      <td>-0.005873</td>\n",
       "      <td>-0.005346</td>\n",
       "      <td>-0.007262</td>\n",
       "      <td>0.049907</td>\n",
       "      <td>-0.011170</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004311</td>\n",
       "      <td>-0.010081</td>\n",
       "      <td>0.008596</td>\n",
       "      <td>-0.008830</td>\n",
       "      <td>-0.007331</td>\n",
       "      <td>-0.015707</td>\n",
       "      <td>0.064298</td>\n",
       "      <td>-0.008365</td>\n",
       "      <td>-0.009674</td>\n",
       "      <td>-1.427382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.015544</td>\n",
       "      <td>-0.032644</td>\n",
       "      <td>-0.132412</td>\n",
       "      <td>-0.034670</td>\n",
       "      <td>-0.017558</td>\n",
       "      <td>-0.004690</td>\n",
       "      <td>-0.006064</td>\n",
       "      <td>-0.008714</td>\n",
       "      <td>0.044191</td>\n",
       "      <td>-0.080955</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005996</td>\n",
       "      <td>-0.016804</td>\n",
       "      <td>-0.009941</td>\n",
       "      <td>-0.011312</td>\n",
       "      <td>-0.029262</td>\n",
       "      <td>-0.020308</td>\n",
       "      <td>-0.059726</td>\n",
       "      <td>0.011893</td>\n",
       "      <td>-0.010554</td>\n",
       "      <td>-1.427382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.025724</td>\n",
       "      <td>-0.059622</td>\n",
       "      <td>0.448252</td>\n",
       "      <td>0.067651</td>\n",
       "      <td>0.100344</td>\n",
       "      <td>0.083437</td>\n",
       "      <td>0.077011</td>\n",
       "      <td>0.046557</td>\n",
       "      <td>0.028375</td>\n",
       "      <td>0.026845</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005520</td>\n",
       "      <td>0.052207</td>\n",
       "      <td>0.005339</td>\n",
       "      <td>0.006736</td>\n",
       "      <td>0.052692</td>\n",
       "      <td>0.030006</td>\n",
       "      <td>0.046512</td>\n",
       "      <td>0.022118</td>\n",
       "      <td>0.002240</td>\n",
       "      <td>-1.427382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.027842</td>\n",
       "      <td>-0.119578</td>\n",
       "      <td>-0.098010</td>\n",
       "      <td>-0.031384</td>\n",
       "      <td>-0.056727</td>\n",
       "      <td>-0.024474</td>\n",
       "      <td>-0.008200</td>\n",
       "      <td>-0.002702</td>\n",
       "      <td>0.069246</td>\n",
       "      <td>0.041450</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004232</td>\n",
       "      <td>-0.014865</td>\n",
       "      <td>-0.015768</td>\n",
       "      <td>0.004216</td>\n",
       "      <td>-0.038378</td>\n",
       "      <td>-0.019654</td>\n",
       "      <td>0.027710</td>\n",
       "      <td>-0.028515</td>\n",
       "      <td>-0.001326</td>\n",
       "      <td>-1.427382</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 83 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   contrib_0_AGE  contrib_1_BILL_AMT1  contrib_10_PAY_0  contrib_11_PAY_2  \\\n",
       "0       0.010583            -0.055527          0.573270          0.153810   \n",
       "1      -0.020959            -0.065462         -0.105277         -0.025094   \n",
       "2      -0.015544            -0.032644         -0.132412         -0.034670   \n",
       "3      -0.025724            -0.059622          0.448252          0.067651   \n",
       "4      -0.027842            -0.119578         -0.098010         -0.031384   \n",
       "\n",
       "   contrib_12_PAY_3  contrib_13_PAY_4  contrib_14_PAY_5  contrib_15_PAY_6  \\\n",
       "0         -0.001514          0.000183         -0.007137         -0.019053   \n",
       "1         -0.016801         -0.005873         -0.005346         -0.007262   \n",
       "2         -0.017558         -0.004690         -0.006064         -0.008714   \n",
       "3          0.100344          0.083437          0.077011          0.046557   \n",
       "4         -0.056727         -0.024474         -0.008200         -0.002702   \n",
       "\n",
       "   contrib_16_PAY_AMT1  contrib_17_PAY_AMT2      ...       \\\n",
       "0             0.035922            -0.002453      ...        \n",
       "1             0.049907            -0.011170      ...        \n",
       "2             0.044191            -0.080955      ...        \n",
       "3             0.028375             0.026845      ...        \n",
       "4             0.069246             0.041450      ...        \n",
       "\n",
       "   contrib_71_InteractionDiv:PAY_0:PAY_AMT3  \\\n",
       "0                                  0.001213   \n",
       "1                                 -0.004311   \n",
       "2                                 -0.005996   \n",
       "3                                 -0.005520   \n",
       "4                                 -0.004232   \n",
       "\n",
       "   contrib_72_NumToCatTE:BILL_AMT1:PAY_0.0  \\\n",
       "0                                 0.078841   \n",
       "1                                -0.010081   \n",
       "2                                -0.016804   \n",
       "3                                 0.052207   \n",
       "4                                -0.014865   \n",
       "\n",
       "   contrib_73_NumToCatTE:BILL_AMT1:LIMIT_BAL:PAY_5:PAY_AMT2:PAY_AMT4.0  \\\n",
       "0                                          -0.005750                     \n",
       "1                                           0.008596                     \n",
       "2                                          -0.009941                     \n",
       "3                                           0.005339                     \n",
       "4                                          -0.015768                     \n",
       "\n",
       "   contrib_75_InteractionMul:BILL_AMT1:PAY_AMT2  \\\n",
       "0                                     -0.002988   \n",
       "1                                     -0.008830   \n",
       "2                                     -0.011312   \n",
       "3                                      0.006736   \n",
       "4                                      0.004216   \n",
       "\n",
       "   contrib_76_ClusterTE:ClusterID96:BILL_AMT1:BILL_AMT4:BILL_AMT5:PAY_0:PAY_AMT2:PAY_AMT4.0  \\\n",
       "0                                           0.066117                                          \n",
       "1                                          -0.007331                                          \n",
       "2                                          -0.029262                                          \n",
       "3                                           0.052692                                          \n",
       "4                                          -0.038378                                          \n",
       "\n",
       "   contrib_77_ClusterTE:ClusterID51:BILL_AMT1:PAY_0:PAY_3.0  \\\n",
       "0                                           0.050699          \n",
       "1                                          -0.015707          \n",
       "2                                          -0.020308          \n",
       "3                                           0.030006          \n",
       "4                                          -0.019654          \n",
       "\n",
       "   contrib_78_InteractionSub:LIMIT_BAL:PAY_4  \\\n",
       "0                                   0.023325   \n",
       "1                                   0.064298   \n",
       "2                                  -0.059726   \n",
       "3                                   0.046512   \n",
       "4                                   0.027710   \n",
       "\n",
       "   contrib_79_ClusterTE:ClusterID54:BILL_AMT1:PAY_3:PAY_AMT2:PAY_AMT6.0  \\\n",
       "0                                          -0.004212                      \n",
       "1                                          -0.008365                      \n",
       "2                                           0.011893                      \n",
       "3                                           0.022118                      \n",
       "4                                          -0.028515                      \n",
       "\n",
       "   contrib_80_NumToCatTE:PAY_0:PAY_AMT2.0  contrib_bias  \n",
       "0                                0.001306     -1.427382  \n",
       "1                               -0.009674     -1.427382  \n",
       "2                               -0.010554     -1.427382  \n",
       "3                                0.002240     -1.427382  \n",
       "4                               -0.001326     -1.427382  \n",
       "\n",
       "[5 rows x 83 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_contributions = h2oai.make_prediction_sync(experiment.key, test_path, \n",
    "                                                      output_margin = False, pred_contribs = True)\n",
    "pred_contributions_path = h2oai.download(prediction_contributions.predictions_csv_path, '.')\n",
    "pred_contributions_table = pd.read_csv(pred_contributions_path)\n",
    "pred_contributions_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will examine the contributions for our first record more closely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contribution</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>contrib_bias</th>\n",
       "      <td>-1.427382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>contrib_10_PAY_0</th>\n",
       "      <td>0.573270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>contrib_23_ClusterTE:ClusterID27:BILL_AMT1:PAY_0:PAY_5:PAY_AMT6.0</th>\n",
       "      <td>0.286473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>contrib_54_NumToCatWoE:PAY_0:PAY_3:PAY_5.0</th>\n",
       "      <td>0.251202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>contrib_11_PAY_2</th>\n",
       "      <td>0.153810</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    contribution\n",
       "contrib_bias                                           -1.427382\n",
       "contrib_10_PAY_0                                        0.573270\n",
       "contrib_23_ClusterTE:ClusterID27:BILL_AMT1:PAY_...      0.286473\n",
       "contrib_54_NumToCatWoE:PAY_0:PAY_3:PAY_5.0              0.251202\n",
       "contrib_11_PAY_2                                        0.153810"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contrib = pd.DataFrame(pred_contributions_table.iloc[0][1:], columns = [\"contribution\"])\n",
    "contrib[\"abs_contribution\"] = contrib.contribution.abs()\n",
    "contrib.sort_values(by=\"abs_contribution\", ascending=False)[[\"contribution\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This customer's `PAY_0` had the greatest impact on their prediction.  Since the contribution is positive, we know that it increases the probability that they will default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Diagnostics on New Data\n",
    "\n",
    "You can use the Python API to also perform model diagnostics on new data. This is equivalent to the **Model Diagnostics** tab in the Web UI. \n",
    "\n",
    "![Model Diagnostics Setup](images/model_diagnostics_setup.png)\n",
    "\n",
    "\n",
    "### 1. Run model diagnostincs on new data with H2OAI model\n",
    "\n",
    "The example below performs model diagnostics on the test dataset but any data with the same columns can be selected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_diagnostics = h2oai.make_model_diagnostic_sync(experiment.key, test.key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'scorer': 'GINI', 'score': 0.6028661606302013},\n",
       " {'scorer': 'MCC', 'score': 0.44234905668200053},\n",
       " {'scorer': 'F05', 'score': 0.5929690808979247},\n",
       " {'scorer': 'F1', 'score': 0.5541978715017738},\n",
       " {'scorer': 'F2', 'score': 0.6477205447010065},\n",
       " {'scorer': 'ACCURACY', 'score': 0.8328333333333333},\n",
       " {'scorer': 'LOGLOSS', 'score': 0.4046366019329677},\n",
       " {'scorer': 'AUC', 'score': 0.8014331637423739},\n",
       " {'scorer': 'AUCPR', 'score': 0.581317661554936}]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[{'scorer': x.scorer, 'score': x.score} for x in test_diagnostics.scores]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the same model diagnostics displayed in the UI: \n",
    "\n",
    "![Model Diagnostics Setup](images/model_diagnostics_complete.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Run Model Interpretation\n",
    "\n",
    "Once we have completed an experiment, we can interpret our H2OAI model.  Model Interpretability is used to provide model transparency and explanations.  More information on Model Interpretability can be found here: http://docs.h2o.ai/driverless-ai/latest-stable/docs/userguide/interpreting.html.\n",
    "\n",
    "\n",
    "### 1. Run Model Interpretation on the Raw Data\n",
    "\n",
    "We can run the model interpretation in the Python client as shown below.  By setting the parameter, `use_raw_features` to True, we are interpreting the model using only the raw features in the data.  This will not use the engineered features we saw in our final model's features to explain the data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mli_experiment = h2oai.run_interpretation_sync(dai_model_key = experiment.key,\n",
    "                                               dataset_key = train.key,\n",
    "                                               target_col = target,\n",
    "                                               use_raw_feature = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is equivalent to clicking **Interpet this Model on Original Features** in the UI once the experiment has completed.\n",
    "\n",
    "![Equivalent Steps in Driverless: View Results](images/experiment_complete_creditcard.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once our interpretation is finished, we can navigate to the MLI tab in the UI to see our interpreted model. \n",
    "\n",
    "![Equivalent Steps in Driverless: MLI List](images/mli_list.png)\n",
    "\n",
    "We can also see the list of interpretations using the Python Client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dukicono']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get list of interpretations\n",
    "mli_list = list(map(lambda x: x.key, h2oai.list_interpretations(offset=0, limit=100)))\n",
    "mli_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2. Run Model Interpretation on External Model Predictions\n",
    "\n",
    "Model Interpretation does not need to be run on a Driverless AI experiment.  We can also train an external model and run Model Interpretability on the predictions. In this next section, we will walk through the steps to interpret an external model.\n",
    "\n",
    "#### Train External Model\n",
    "\n",
    "We will begin by training a model with scikit-learn.  Our end goal is to use Driverless AI to interpret the predictions made by our scikit-learn model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dataset must be located where Python client is running\n",
    "train_pd = pd.read_csv(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "              presort='auto', random_state=10, subsample=1.0, verbose=0,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "predictors = list(set(train_pd.columns) - set([target]))\n",
    "\n",
    "gbm_model = GradientBoostingClassifier(random_state=10)\n",
    "gbm_model.fit(train_pd[predictors], train_pd[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.38111179, 0.61888821],\n",
       "       [0.44396186, 0.55603814],\n",
       "       [0.91738328, 0.08261672],\n",
       "       [0.88780536, 0.11219464],\n",
       "       [0.80028008, 0.19971992]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = gbm_model.predict_proba(train_pd[predictors])\n",
    "predictions[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpret on External Predictions\n",
    "\n",
    "Now that we have the predictions from our scikit-learn GBM model, we can call Driverless AI's **`h2o_ai.run_interpretation_sync`** to create the interpretation screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_gbm_path = \"./CreditCard-train-gbm_pred.csv\"\n",
    "predictions = pd.concat([train_pd, pd.DataFrame(predictions[:, 1], columns = [\"p1\"])], axis = 1)\n",
    "predictions.to_csv(path_or_buf=train_gbm_path, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_gbm_pred = h2oai.upload_dataset_sync(train_gbm_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mli_external = h2oai.run_interpretation_sync(dai_model_key = \"\", # no experiment key\n",
    "                                             dataset_key = train_gbm_pred.key,\n",
    "                                             target_col = target,\n",
    "                                             prediction_col = \"p1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also run Model Interpretability on an external model in the UI as shown below:\n",
    "\n",
    "![Equivalent Steps in Driverless: MLI External Model](images/mli_external.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fawinusa', 'dukicono']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get list of interpretations\n",
    "mli_list = list(map(lambda x: x.key, h2oai.list_interpretations(offset=0, limit=100)))\n",
    "mli_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Scoring Pipelines\n",
    "\n",
    "In our last section, we will build the scoring pipelines from our experiment.  There are two scoring pipeline options: \n",
    "\n",
    "* Python Scoring Pipeline: requires Python runtime\n",
    "* MOJO Scoring Pipeline: requires Java runtime\n",
    "\n",
    "Documentation on the scoring pipelines is provided here: http://docs.h2o.ai/driverless-ai/latest-stable/docs/userguide/python-mojo-pipelines.html.\n",
    "\n",
    "![Equivalent Steps in Driverless: View Results](images/experiment_complete_creditcard.png)\n",
    "\n",
    "The experiment screen shows two scoring pipeline buttons: **Download Python Scoring Pipeline** or **Build MOJO Scoring Pipeline**.  Driverless AI determines if any scoring pipeline should be automatically built based on the config.toml file.  In this example, we have run Driverless AI with the settings: \n",
    "\n",
    "```\n",
    "# Whether to create the Python scoring pipeline at the end of each experiment\n",
    "make_python_scoring_pipeline = true\n",
    "\n",
    "# Whether to create the MOJO scoring pipeline at the end of each experiment\n",
    "# Note: Not all transformers or main models are available for MOJO (e.g. no gblinear main model)\n",
    "make_mojo_scoring_pipeline = false\n",
    "```\n",
    "\n",
    "Therefore, only the Python Scoring Pipeline will be built by default.\n",
    "\n",
    "### 1. Build Python Scoring Pipeline\n",
    "\n",
    "The Python Scoring Pipeline has been built by default based on our config.toml settings.  We can get the path to the Python Scoring Pipeline in our experiment object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'h2oai_experiment_hesenifo/scoring_pipeline/scorer.zip'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment.scoring_pipeline_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also build the Python Scoring Pipeline - this is useful if the **`make_python_scoring_pipeline`** option was set to false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "python_scoring_pipeline = h2oai.build_scoring_pipeline_sync(experiment.key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'h2oai_experiment_hesenifo/scoring_pipeline/scorer.zip'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "python_scoring_pipeline.file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will download the scoring pipeline zip file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./scorer.zip'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h2oai.download(python_scoring_pipeline.file_path, dest_dir=\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Build MOJO Scoring Pipeline\n",
    "\n",
    "The MOJO Scoring Pipeline has not been built by default because of our config.toml settings.  We can build the MOJO Scoring Pipeline using the Python client.  This is equivalent to selecting the **Build MOJO Scoring Pipeline** on the experiment screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mojo_scoring_pipeline = h2oai.build_mojo_pipeline_sync(experiment.key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'h2oai_experiment_hesenifo/mojo_pipeline/mojo.zip'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mojo_scoring_pipeline.file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can download the scoring pipeline zip file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./mojo.zip'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h2oai.download(mojo_scoring_pipeline.file_path, dest_dir=\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Once the MOJO Scoring Pipeline is built, the **Build MOJO Scoring Pipeline** changes to **Download MOJO Scoring Pipeline**.\n",
    "\n",
    "\n",
    "\n",
    "![Equivalent Steps in Driverless: Download MOJO](images/download_mojo.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
